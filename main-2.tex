\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{indentfirst}

\title{Search Engine}
\author{Alessandra Cid, Lucas Emanuel Resck  
and Lucas Moschen}
\date{June 2019}

\begin{document}

\maketitle

\begin{abstract}
For this project our group developed a search engine that searches for a word, or multiple words, in the English portion of the Wikipedia corpus. In order to do that, we constructed a trie in C++ to use as our data structure. We used Python to pre-process the corpus and then inserted it into the trie. Our search is also done in C++. If we have more than one word, we compare the pages that are common to all words and only return them. 
\end{abstract}

\section*{Relevant references}
% Link to a github repository where your source code is located. They will have to create a TAG with the following name "FINAL". I will verify that the date of creation of the tag is ​x otherwise it will not grade your work. Moreover, this repository should have a README, where it clearly explains how to run your program, datasets, etc. If I can't run your program, then it won't be evaluated either. Those instructions should also be in this document.
This is the link to the group's Github repository containing all of the project's 
source code: xx. In order to run the program you should...

%Link to a video explaining your whole project and showing me how it works. You can take inspiration from the following ​video​.
This is the link to the group's video: xx. 

\section*{Description}
The Wikipedia corpus used in this project was downloaded as several different text files. Each file contained one or more Wikipedia pages. The first step in the project was to pre-process, in Python, this data. Non ASCII symbols were removed, and many ASCII symbols too, such as ``+'', ``\{'' and ``]'', but all of the numbers and the symbols ``\%'', ``\&'', ``-'', ``@'' and `` ' '' were maintained. All of the accents from the words that had them were also removed and the letters in upper case were transformed into lower case. 

The way the different Wikipedia pages were stored in each text file was also changed. Taking advantage of the ``<doc id'' tag identifying the begging of each page, we separated them into individual text files. 

For the data structure, the group chose to build a trie. A trie is a tree that stores different words. In this case, all the words in the Wikipedia corpus. Each letter of a word in the trie is the child of the letter that came before it in the word. 
For this project, the trie was constructed going through the text files that corresponded to the Wikipedia pages. Each word in the file that wasn't already in the trie by the time it was reached, was inserted into it. In order to identify the Wikipedia pages that contained each word, a pointer to an array named docs was placed in each node. These arrays stored numbers identifying each of the pages in which a certain word appears. They could be accessed through the node related to the last letter in the word. 

To build this in C++, pointers were used. Each pointer had 128 potential ``child'' pointers corresponding to a specific ASCII character. If a person wants to verify if a specific word is in the trie, they can start from the root of the trie (pRoot) and visit the child (pChild) corresponding to the ASCII character of each of the letters in the word. If they were able to reach the node corresponding to the last letter in the word, it means that in at least some Wikipedia page that word exists. Accessing the array docs from that point allows the person to see in which Wikipedia pages that word exists.

%maybe make an image with an example of a trie with a vector indicating some pages

In order to access this trie outside the scope of the original code in which it was constructed, the group performed a serialization of the trie, inserting it into a text file. This serialization started from the root and accessed the child nodes. If the node existed, the ASCII character corresponding to that node was inserted into the file. A dot (``.'') indicates that the number before corresponds to a node. A plus sign (``+'') followed by a number indicates the size of the array of documents, if it contained any page. In this case, the numbers corresponding to the pages were separated by commas (``,''). After this, that node's child was accessed and the same process was performed again. When a node did not have any more child nodes a ``-'' sign was written in the document. Following the logic through which the serialization was developed, a deserialization function was built. 

In the search environment, the first step of the program is to perform the deserialization. After that, the user can enter a query.  The query is pre-processed, removing accents and lowering the case of letters in upper case. Then, each word in the query is identified and searched in the trie, by accessing the child of each of the word's letters in the trie. If that word exists in one or more Wikipedia page, the documents array containing those pages is returned. If that word does not exist, the expression with the same first letters that has the highest number of mentions in Wikipedia pages is returned. This way, the search returns a popular word similar to the one searched by the user. 

If a query has more than one word, each time the program searches for a new word it compares the previous document vector that it had with the new searched one and only keeps the pages that are common to both. As the page numbers are ordered in each vector, this is done in linear time. 

After the search ends, the program returns the number of results that were found and identifies the title of each page found in the search. The title of the 20 first results are then displayed in alphabetical order. The user is able open in details an specific page that returned as a result or to see the title of 20 more pages found in the search. 

%What did you do? What data structure did you use? Did you do any pre-processing?

\section*{Results}
The indexing takes approximately xx minutes. Originally, the group downloaded xx documents. Each Wikipedia page in those documents was transformed into an individual file. That resulted in xx files, corresponding to xx Wikipedia pages. In order to store the trie xx of space was used. The other tasks involved in the search took xx space. Searching for a certain word takes xx seconds. 
% How long does the indexing take? How many documents did you index? How much space did you use? How long does the consultation take? Make some graphs showing the results.

\section*{Limitations}
Although the group was able to successfully perform the task of searching the Wikipedia corpus, there are some limitations to the final result. The serialization and the deserialization are processes that take time and could be done faster. A binary document could be used for this and so could other C++ packages that make it more efficient. The alternative result suggestion, when a word is typed wrong, could also be improved. The place where letters are located in the keyboard could indicate common mistakes between two keys. A list could also be made of other common mistakes made by users. The group could also have taken advantage of algorithms, such as the minimum edit distance, to identify similar words. The interface could have been xx.  

%what about inplace intersection
%interface, serialization, typo correction

\section*{Future work}
For future work it would be interesting to improve on the mentioned limitations of the project, such as the serialization time, the word correction and the interface. The project's database could also be expanded, including searches in the Wikipedia corpus of different languages. We could also xx. 
%ideas: faster search, faster serialization, better typo correction, wiki in different languages, interface melhor

\section*{Conclusion}

\section*{Distribution of work in the group}
The main decisions for the project were taken by the group and all members helped, by making suggestions, thinking of new approaches and fixing bugs, on parts of the project that they were not primarily responsible for. Lucas Moschen was responsible for pre-processing the data, building the overall search interface and writing the function for alternative result suggestion in case of misspelled queries. Lucas Emanuel Resck built the data structure for the trie. Lucas Moschen and Alessandra worked on initial versions of the serialization and Lucas Resck was responsible for the final one. Alessandra built the word identification for the query and the search in the trie and wrote this report for the project. xx edited the video.

%Distribution of work: Indicate clearly which parts of the work were done by each of the members of the group.
\end{document}
